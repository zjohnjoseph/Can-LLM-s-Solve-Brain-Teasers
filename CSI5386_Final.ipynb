{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lAVO3MJ6FDIL",
    "outputId": "0b37256a-1581-4a77-c6c0-e9c49876b4c7"
   },
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from datasets import Dataset\n",
    "from datetime import datetime\n",
    "import evaluate\n",
    "import gdown\n",
    "import logging\n",
    "from logging import debug, info, warning, error\n",
    "import numpy as np\n",
    "import os\n",
    "from pprint import pformat\n",
    "import re\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoModelForMultipleChoice,\n",
    "    BitsAndBytesConfig,\n",
    "    set_seed,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from transformers.tokenization_utils_base import PreTrainedTokenizerBase, PaddingStrategy\n",
    "from typing import Iterator, Optional, Sequence, Union\n",
    "\n",
    "LOG_LEVEL = 'DEBUG'\n",
    "LOG_TO_FILE = True\n",
    "log_folder='logs'\n",
    "\n",
    "if not os.path.exists(log_folder):\n",
    "    os.mkdir(log_folder)\n",
    "\n",
    "logging.basicConfig(\n",
    "  level=logging.getLevelName(LOG_LEVEL), \n",
    "  format='%(message)s', \n",
    "  force=True,\n",
    "  filename=f'{log_folder}/{datetime.now()}' if LOG_TO_FILE else None,\n",
    "  filemode='a'\n",
    ")\n",
    "\n",
    "data_folder = 'data'\n",
    "sp_train_path = f'./{data_folder}/SP-train.npy'\n",
    "sp_eval_path = f'./{data_folder}/SP_eval_data_for_practice.npy'\n",
    "wp_train_path = f'./{data_folder}/WP-train.npy'\n",
    "wp_eval_path = f'./{data_folder}/WP_eval_data_for_practice.npy'\n",
    "\n",
    "# https://drive.google.com/drive/u/0/folders/1BNnhh2HsxId3bWQ6_A4Mou4x44Bm43VY\n",
    "if not os.path.exists(data_folder):\n",
    "    gdown.download_folder(id='1BNnhh2HsxId3bWQ6_A4Mou4x44Bm43VY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7cR0S14uGSf4",
    "outputId": "19cc43f9-83b1-4d3b-c167-030f878d5701"
   },
   "outputs": [],
   "source": [
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "SEED = 42\n",
    "\n",
    "TEST_SPLIT_RATIO = 0.1\n",
    "VALIDATE_SPLIT_RATIO = 0.2\n",
    "\n",
    "set_seed(SEED)\n",
    "\n",
    "def get_data(path: str) -> Iterator[dict]:\n",
    "  \"\"\"\n",
    "  Load the data from the given path and return a list of dicts\n",
    "  Training data has all of the fields, while test data only contains question and choice_list\n",
    "  NOTE: there are a couple items in the dataset that have incorrect schemas... SP-209 (not valid), SP-219-221 (label is float)\n",
    "  \"\"\"\n",
    "  entries = [{k: (v if isinstance(v, Sequence) else str(v)) for k, v in entry.items()} for entry in np.load(path, allow_pickle=True)]\n",
    "    for entry in entries:\n",
    "        if 'label' in entry:\n",
    "            entry['label'] = int(entry['label'])\n",
    "\n",
    "    for i, choice in enumerate(['a', 'b', 'c', 'd']):\n",
    "        entry[choice] = entry['choice_list'][i]\n",
    "    return entries\n",
    "\n",
    "# NOTE: the 'eval' test set is just to test formatting, we can't actually use it for anything since it doesn't have\n",
    "# labels, so we need to create our validation/test sets manually from the 'train' set\n",
    "sp_dataset = Dataset.from_list(get_data(sp_train_path)).shuffle(seed=SEED).train_test_split(TEST_SPLIT_RATIO)\n",
    "validation = sp_dataset['test']\n",
    "sp_dataset = sp_dataset['train'].train_test_split(VALIDATE_SPLIT_RATIO)\n",
    "sp_dataset['validation'] = validation\n",
    "\n",
    "debug(sp_dataset)\n",
    "debug(sp_dataset['train'][0])\n",
    "debug(sp_dataset['validation'][0])\n",
    "debug(sp_dataset['test'][0])\n",
    "\n",
    "wp_dataset = Dataset.from_list(get_data(wp_train_path)).shuffle(seed=SEED).train_test_split(TEST_SPLIT_RATIO)\n",
    "validation = wp_dataset['test']\n",
    "wp_dataset = wp_dataset['train'].train_test_split(VALIDATE_SPLIT_RATIO)\n",
    "wp_dataset['validation'] = validation\n",
    "\n",
    "debug(wp_dataset)\n",
    "debug(wp_dataset['train'][0])\n",
    "debug(wp_dataset['validation'][0])\n",
    "debug(wp_dataset['test'][0])\n",
    "\n",
    "sp_test_dataset = Dataset.from_list(get_data(sp_eval_path))\n",
    "wp_test_dataset = Dataset.from_list(get_data(wp_eval_path))\n",
    "\n",
    "debug(sp_test_dataset)\n",
    "debug(sp_test_dataset[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0aJnUAB6PyPB"
   },
   "source": [
    "## MultipleChoice\n",
    "\n",
    "This is a sample of how it would work with models that support AutoModelForMultipleChoice, modified from https://huggingface.co/docs/transformers/en/tasks/multiple_choice\n",
    "\n",
    "NOTE: can't seem to get BERT to load on the GPU for this task..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8NoAicQbOqqj",
    "outputId": "2ecada0b-6881-45ce-f36b-cb9de16b920e"
   },
   "outputs": [],
   "source": [
    "%%script echo skipping\n",
    "\n",
    "model_id = 'google-bert/bert-base-uncased'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForMultipleChoice.from_pretrained(model_id)\n",
    "\n",
    "def preprocess(examples, **fn_kwargs):\n",
    "  # for each example in the batch, convert to 4 question/option pairs\n",
    "  # we use 'choice_list' as it's in the order that corresponds to index in 'label'\n",
    "    first_sentences = [[question] * 4 for question in examples['question']]\n",
    "    second_sentences = [[option for option in examples['choice_list'][i]] for i in range(len(examples['question']))]\n",
    "  \n",
    "    debug(first_sentences[0])\n",
    "    debug(second_sentences[0])\n",
    "  \n",
    "    # flatten to allow tokenizing\n",
    "    first_sentences = sum(first_sentences, [])\n",
    "    second_sentences = sum(second_sentences, [])\n",
    "  \n",
    "    # truncate here, but dynamically pad to longest in batch if needed later  in the data collator\n",
    "    tokenized_examples = fn_kwargs['tokenizer'](first_sentences, second_sentences, truncation=True)\n",
    "  \n",
    "    # unflatten for use in inference\n",
    "    return {k: [v[i : i + 4] for i in range(0, len(v), 4)] for k, v in tokenized_examples.items()}\n",
    "\n",
    "tokenized_sp_dataset = sp_dataset.map(preprocess, batched=True, fn_kwargs={'tokenizer': tokenizer})\n",
    "debug(tokenized_sp_dataset['train'][0])\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorForMultipleChoice:\n",
    "    \"\"\"\n",
    "    Data collator that will dynamically pad the inputs for multiple choice received.\n",
    "    \"\"\"\n",
    "\n",
    "    tokenizer: PreTrainedTokenizerBase\n",
    "    padding: Union[bool, str, PaddingStrategy] = True\n",
    "    max_length: Optional[int] = None\n",
    "    pad_to_multiple_of: Optional[int] = None\n",
    "\n",
    "    def __call__(self, features):\n",
    "        label_name = \"label\" if \"label\" in features[0].keys() else \"labels\"\n",
    "        labels = [feature.pop(label_name) for feature in features]\n",
    "        batch_size = len(features)\n",
    "        num_choices = len(features[0][\"input_ids\"])\n",
    "        flattened_features = [\n",
    "            [{k: v[i] for k, v in feature.items()} for i in range(num_choices)] for feature in features\n",
    "        ]\n",
    "        flattened_features = sum(flattened_features, [])\n",
    "\n",
    "        batch = self.tokenizer.pad(\n",
    "            flattened_features,\n",
    "            padding=self.padding,\n",
    "            max_length=self.max_length,\n",
    "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        batch = {k: v.view(batch_size, num_choices, -1) for k, v in batch.items()}\n",
    "        batch[\"labels\"] = torch.tensor(labels, dtype=torch.int64)\n",
    "        return batch\n",
    "\n",
    "accuracy = evaluate.load('accuracy')\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return accuracy.compute(predictions=predictions, references=labels)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./my_awesome_model\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.01\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_sp_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_sp_dataset[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorForMultipleChoice(tokenizer=tokenizer),\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kPZjuzB34WZy",
    "outputId": "bf5fe8fe-91ea-4ae5-9e1f-57b177df1eea"
   },
   "outputs": [],
   "source": [
    "%%script echo skipping\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('my_awesome_model')\n",
    "model = AutoModelForMultipleChoice('my_awesome_model')\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=TrainingArguments(output_dir='./results', per_device_eval_batch_size=64),\n",
    "    eval_dataset=tokenized_sp_dataset['test'],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorForMultipleChoice(tokenizer=tokenizer),\n",
    "    compute_metrics=compute_metrics\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xspK_jXRP40q"
   },
   "source": [
    "## Most Likely Response\n",
    "\n",
    "Similar to above, we split an MCQ into 4 question/option pairs, and then we use a CausalLM model to compute the likelihood that the model would have generated the answer. No fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wnt48WdJ-uEh",
    "outputId": "c4a29adc-ec5a-4493-8cee-59a622a330c3"
   },
   "outputs": [],
   "source": [
    "%%script echo skipping\n",
    "\n",
    "model_id = 'gpt2'\n",
    "\n",
    "def calculate_likelihood(model, tokenizer, question, option):\n",
    "    tokens = tokenizer.encode(question + option, return_tensors='pt')\n",
    "    prompt_length = len(tokenizer.encode(question))\n",
    "    input_ids = tokens[:, :-1]\n",
    "    target_ids = tokens[:, 1:].clone()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, labels=target_ids)\n",
    "        log_likelihood = outputs.loss * target_ids.size(1)\n",
    "    return log_likelihood.item()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "\n",
    "predictions = []\n",
    "labels = []\n",
    "for example in sp_dataset['train'].select(range(4)):\n",
    "    pairs = [[example['question'], example['choice_list'][i]] for i in range(len(example['choice_list']))]\n",
    "    likelihoods = [calculate_likelihood(model, tokenizer, question, option) for question, option in pairs]\n",
    "    max_likelihood = max(likelihoods)\n",
    "    max_index = likelihoods.index(max_likelihood)\n",
    "    predictions.append(max_index)\n",
    "    labels.append(example['label'])\n",
    "\n",
    "debug(f'{predictions[0]}, {labels[0]}')\n",
    "info(f\"Accuracy: {accuracy_score(labels, predictions)}\")\n",
    "info(f\"F1: {f1_score(labels, predictions, average='weighted')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k-7UfaAQfmF1"
   },
   "source": [
    "## Prompt Engineering\n",
    "\n",
    "This approach tries to use large LLMs and use varying prompting strategies to answer with the best option"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S0HXRcuTtgGD"
   },
   "outputs": [],
   "source": [
    "from string import Template\n",
    "\n",
    "message_templates = {\n",
    "    'manual-basic-mcq': Template('''\n",
    "Which of the following options answers this multiple-choice question: $question\n",
    "A) $a\n",
    "B) $b\n",
    "C) $c\n",
    "D) $d\n",
    "Provide the correct option as A, B, C, or D.\n",
    "Answer: '''),\n",
    "\n",
    "    'manual-basic-riddle': Template('''\n",
    "Which of the following options answers this multiple-choice riddle: $question\n",
    "A) $a\n",
    "B) $b\n",
    "C) $c\n",
    "D) $d\n",
    "Provide the correct option as A, B, C, or D.\n",
    "This is a riddle so the answer may involve word play or lateral thinking.\n",
    "Answer: '''),\n",
    "    \n",
    "    'manual-chain-of-thought': Template('''\n",
    "Which of the following options answers this multiple-choice riddle: $question\n",
    "A) $a\n",
    "B) $b\n",
    "C) $c\n",
    "D) $d\n",
    "Provide the correct option as A, B, C, or D.\n",
    "This is a riddle so the answer may involve word play or lateral thinking.\n",
    "Explain how you deduce the answer step by step.\n",
    "Answer: '''),\n",
    "\n",
    "    'llama-3-prompt-generation': Template('''\n",
    "Riddle: $question\n",
    "Options:\n",
    "A) $a\n",
    "B) $b\n",
    "C) $c\n",
    "D) $d\n",
    "Choose the correct answer:\n",
    "(Select one of the above options by typing A, B, C, or D)'''),\n",
    "    \n",
    "    'gemini-prompt-generation': Template('''\n",
    "Riddle:\n",
    "\n",
    "$question\n",
    "\n",
    "Possible Answers:\n",
    "\n",
    "    A. $a\n",
    "    B. $b\n",
    "    C. $c\n",
    "    D. $d\n",
    "\n",
    "Instructions:\n",
    "\n",
    "    Read the riddle carefully and consider the meaning of the words used.\n",
    "    Analyze the possible answers and identify any clues that point towards a specific answer.\n",
    "    Based on your understanding of the riddle and the logic behind the answer choices, select the option that best fits the riddle's description.\n",
    " \n",
    "'''),\n",
    "\n",
    "    'chatgpt-4-prompt-generation': Template('''\n",
    "Riddle: $question\n",
    "\n",
    "Options:\n",
    "A) $a\n",
    "B) $b\n",
    "C) $c\n",
    "D) $d\n",
    "\n",
    "Please select the correct option that answers the riddle above. ''')\n",
    "}\n",
    "\n",
    "model_configs = {\n",
    "  'gemma-7b-it': {\n",
    "    'id': 'google/gemma-7b-it',\n",
    "    'normalize': lambda response: response.split('\\nmodel\\n')[1].strip()\n",
    "  },\n",
    "  'llama-3-8b-it': {\n",
    "    'id': 'meta-llama/Meta-Llama-3-8B-Instruct',\n",
    "    'normalize': lambda response: response.split('assistant\\n')[1].strip().replace('\\n', ' ')\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "model_key = 'llama-3-8b-it' # Change to gemma-7b-it\n",
    "model_config = model_configs[model_key]\n",
    "\n",
    "# Mem savings: we can either load the model in 16bit precision and use 'device_map=auto' or in less than 16 bit with bitsandbytes\n",
    "# low_cpu_mem_usage=True prevents overhead of having a randomly initialized weight matrix created first and then updating with the model params\n",
    "# device_map='auto' spread the models weights into GPU, CPU, and then to disk, and also sets low_cpu_mem_usage\n",
    "# torch_dtype=torch.float16 reduces memory usage by half, since most models are created using 32 bit precision params\n",
    "# quantization_config allows for setting parameter precision less than 16 bits.\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_config['id'], quantization_config=quantization_config, low_cpu_mem_usage=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_config['id'], device_map='auto', torch_dtype=torch.float16)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_config['id'], padding_side=\"left\")\n",
    "tokenizer.pad_token = tokenizer.pad_token if tokenizer.pad_token else tokenizer.eos_token\n",
    "debug(f'Pad token: {tokenizer.pad_token}')\n",
    "debug(f'EOS token: {tokenizer.eos_token}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 767,
     "referenced_widgets": [
      "340433caaa774374abf5d922f27e984f",
      "5352a7b96c414492804772c6ca4ee2b5",
      "a98c568cd3dc4d4e9644702c710dce14",
      "9517aed527e84baa92a5ea6ea404e121",
      "601ce15f87204fdf92042039da453d3b",
      "47f341f780e54e979d1c4373d93160b5",
      "661ccfdaec4a481ea5bed718b9832718",
      "fa08ab1e6bb14c2eb40994e92dcf3ffe",
      "90ee5e791d7e4ef9b2f9b4bdc6bcc6b8",
      "fd6cb49a7a684ab09c54504c677e2ac5",
      "c7d5e5f2e2264a369388a51be285cb81"
     ]
    },
    "id": "SWu0fYgYfwSX",
    "outputId": "0af77d77-c55e-43c1-d6af-5326eac1a518"
   },
   "outputs": [],
   "source": [
    "prog = re.compile('^(?:([ABCD])[.)]|(?![ABCD][.)]).*\\s([ABCD])[ .)]|.*: ([ABCD]))')\n",
    "\n",
    "for template_id in message_templates.keys():\n",
    "    \n",
    "    info(f'Using template: {template_id}')\n",
    "    predictions = []\n",
    "    labels = []\n",
    "    \n",
    "    for example in wp_dataset['test']: # Also Change to sp_dataset.\n",
    "        title = f'EVAL-{model_key}-{template_id}-wp'\n",
    "\n",
    "        logging.basicConfig(\n",
    "          level=logging.getLevelName(LOG_LEVEL),\n",
    "          format='%(message)s',\n",
    "          force=True,\n",
    "          filename=f'{log_folder}/{title}' if LOG_TO_FILE else None,\n",
    "          filemode='a'\n",
    "        )\n",
    "        info('-------------------')\n",
    "\n",
    "        messages = [{\"role\": \"user\", \"content\": message_templates[template_id].substitute(**example)}]\n",
    "        debug(messages[0]['content'])\n",
    "        model_inputs = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors='pt').to(DEVICE)\n",
    "        generated_ids = model.generate(\n",
    "            model_inputs,\n",
    "            max_new_tokens=1000 if template_id == 'manual-chain-of-thought' else 750,\n",
    "            do_sample=True, # greedy vs. sampling\n",
    "            top_k = 50,\n",
    "            top_p = 0.95,\n",
    "            \n",
    "        )\n",
    "\n",
    "        response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "        response = response.replace('**', '') # Some models seem to like outputting with ** for formatting purposes...\n",
    "\n",
    "        # do any model specific processing if required\n",
    "        response = model_config['normalize'](response) if 'normalize' in model_config else response\n",
    "\n",
    "        debug(f'Response: {response}')\n",
    "\n",
    "        # heuristics to get the answer from the response, first check if there's a regex match, and then\n",
    "        # check if the text of an option is contained 2s in the response, i.e. 'the answer is <option text>'\n",
    "        matches = prog.search(response)\n",
    "        option = next((group for group in matches.groups() if group), None) if matches else None\n",
    "        if not option:\n",
    "            warning(f'No regex match')\n",
    "\n",
    "            for i, choice in enumerate(['A', 'B', 'C', 'D']):\n",
    "                option_text = example['choice_list'][i].lower().rstrip('.')\n",
    "                response_text = response.lower()\n",
    "                if option_text in response_text:\n",
    "                    option = choice\n",
    "                    break\n",
    "\n",
    "        if not option:\n",
    "            info('Could not get option, skipping...')\n",
    "            continue\n",
    "\n",
    "        answer = option\n",
    "        info(f'Answer: {answer}')\n",
    "        answer_index = ord(answer) - ord('A') # get the index of the answer\n",
    "        if 'label' in example:\n",
    "            correct_index = example['label']\n",
    "            correct = ['A', 'B', 'C', 'D'][correct_index]\n",
    "            info(f'Correct: {correct}')\n",
    "            is_correct = answer_index == correct_index\n",
    "            info(f\"{'Right' if is_correct else 'Wrong'}!\\n\")\n",
    "            labels.append(correct_index)\n",
    "        predictions.append(answer_index)\n",
    "\n",
    "    info('-------------------')\n",
    "    with open(f'{title}.txt', 'w') as f:\n",
    "        f.write(pformat({'predictions': predictions, 'labels': labels}))\n",
    "\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    f1 = f1_score(labels, predictions, average='weighted')\n",
    "    info(f\"Accuracy: {accuracy}\\nF1: {f1}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "340433caaa774374abf5d922f27e984f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_5352a7b96c414492804772c6ca4ee2b5",
       "IPY_MODEL_a98c568cd3dc4d4e9644702c710dce14",
       "IPY_MODEL_9517aed527e84baa92a5ea6ea404e121"
      ],
      "layout": "IPY_MODEL_601ce15f87204fdf92042039da453d3b"
     }
    },
    "47f341f780e54e979d1c4373d93160b5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5352a7b96c414492804772c6ca4ee2b5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_47f341f780e54e979d1c4373d93160b5",
      "placeholder": "​",
      "style": "IPY_MODEL_661ccfdaec4a481ea5bed718b9832718",
      "value": "config.json: 100%"
     }
    },
    "601ce15f87204fdf92042039da453d3b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "661ccfdaec4a481ea5bed718b9832718": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "90ee5e791d7e4ef9b2f9b4bdc6bcc6b8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "9517aed527e84baa92a5ea6ea404e121": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fd6cb49a7a684ab09c54504c677e2ac5",
      "placeholder": "​",
      "style": "IPY_MODEL_c7d5e5f2e2264a369388a51be285cb81",
      "value": " 627/627 [00:00&lt;00:00, 20.1kB/s]"
     }
    },
    "a98c568cd3dc4d4e9644702c710dce14": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fa08ab1e6bb14c2eb40994e92dcf3ffe",
      "max": 627,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_90ee5e791d7e4ef9b2f9b4bdc6bcc6b8",
      "value": 627
     }
    },
    "c7d5e5f2e2264a369388a51be285cb81": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "fa08ab1e6bb14c2eb40994e92dcf3ffe": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fd6cb49a7a684ab09c54504c677e2ac5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
